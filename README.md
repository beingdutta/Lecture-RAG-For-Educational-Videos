# ğŸ“ Grounded Video Question Answering for Lecture Videos  
### A Self-Refinement Framework with OCR-Grounded Reasoning

> **TL;DR**  
> Lecture videos (slides + blackboard + face cam) break standard Video-LLMs.  
> This repository implements a **grounding-aware self-refinement framework** that reduces hallucinations, supports algorithmic reasoning, and works on real educational videos.

---

## ğŸš¨ The Problem

Most Video-Language Models (Video-LLMs) are evaluated on:
- short clips,
- natural scenes,
- action recognition,
- captioning benchmarks.

**Lecture videos are fundamentally different.**

They typically contain:
- dense slides with text,
- handwritten blackboard content,
- algorithm pseudocode,
- equations,
- partial visual context,
- long durations with sparse visual change.

As a result, existing approaches fail in **two critical ways**:

### âŒ Failure Mode 1: Hallucination
Models confidently answer **from prior knowledge**, even when:
- the video is blank,
- the relevant slide is not sampled,
- the text is unreadable.

### âŒ Failure Mode 2: Over-Abstention
When strict grounding is enforced, models respond with:

> *"The answer cannot be determined from the video."*

â€”even when the answer is **clearly derivable** from:
- algorithm steps,
- equations,
- procedural descriptions shown on slides.

This is especially severe for:
- *â€œwhyâ€* questions,
- algorithm explanations,
- time/space complexity discussions,
- reasoning over steps.

ğŸ‘‰ **No existing open-source framework robustly handles this for educational / lecture-style videos.**

---

## ğŸ’¡ Key Insight

> **Grounding is not binary for lecture videos.**

There are **three distinct grounding regimes**:

| Grounding Type | Example | Should Answer? |
|---------------|--------|----------------|
| **Explicit grounding** | â€œWhat is written on the slide?â€ | âœ… |
| **Derivable-from-steps grounding** | â€œWhy initialize keys to âˆ in Primâ€™s algorithm?â€ | âœ… |
| **Theoretical / external grounding** | â€œWhy does Prim always produce an MST?â€ | âŒ |

Most systems collapse everything into **SUPPORTED vs UNSUPPORTED**, which is incorrect for algorithmic lectures.

---

## ğŸ§  What This Repository Implements

This repository introduces a **Grounding-Aware Self-Refinement Framework** for lecture videos.

---

## ğŸ§© Core Components

### 1ï¸âƒ£ OCR-First Grounding

- OCR is treated as **primary evidence**, not a helper signal.
- The model is explicitly instructed to use:
  - OCR text
  - clearly visible visual evidence
- External knowledge is disallowed unless it is **logically derivable from shown steps**.

This prevents hallucinations while still allowing explanation-based answers.

---

### 2ï¸âƒ£ Hybrid OCR Retrieval (Query-Aware)

- OCR is run on uniformly sampled frames.
- A **hybrid search** (semantic similarity + keyword overlap) selects only OCR segments relevant to the question.
- This removes noise from:
  - instructor bios,
  - course outlines,
  - unrelated slides,
  - decorative content.

---

### 3ï¸âƒ£ Grounding-Aware Self-Refinement

Inspired by SELF-REFINE, but **adapted for multimodal grounding**.

Each iteration consists of:
1. **Answer generation**
2. **Grounding feedback classification**
3. **Answer refinement**

Instead of binary feedback, answers are classified into:

- `SUPPORTED`
- `DERIVABLE_FROM_STEPS`
- `PARTIALLY_SUPPORTED`
- `UNSUPPORTED`

This enables:
- correct algorithm explanations,
- grounded reasoning over steps,
- correction of partial hallucinations,
- abstention only when truly necessary.

---

### 4ï¸âƒ£ Robustness to Black-Screen & Failure Cases

- On videos with no usable OCR or visual evidence, the system **correctly abstains**.
- This prevents the common failure of confident but ungrounded answers.

---

## ğŸ§  Architecture Overview

# ğŸ“ Grounded Video Question Answering for Lecture Videos  
### A Self-Refinement Framework with OCR-Grounded Reasoning

> **TL;DR**  
> Lecture videos (slides + blackboard + face cam) break standard Video-LLMs.  
> This repository implements a **grounding-aware self-refinement framework** that reduces hallucinations, supports algorithmic reasoning, and works on real educational videos.

---

## ğŸš¨ The Problem

Most Video-Language Models (Video-LLMs) are evaluated on:
- short clips,
- natural scenes,
- action recognition,
- captioning benchmarks.

**Lecture videos are fundamentally different.**

They typically contain:
- dense slides with text,
- handwritten blackboard content,
- algorithm pseudocode,
- equations,
- partial visual context,
- long durations with sparse visual change.

As a result, existing approaches fail in **two critical ways**:

### âŒ Failure Mode 1: Hallucination
Models confidently answer **from prior knowledge**, even when:
- the video is blank,
- the relevant slide is not sampled,
- the text is unreadable.

### âŒ Failure Mode 2: Over-Abstention
When strict grounding is enforced, models respond with:

> *"The answer cannot be determined from the video."*

â€”even when the answer is **clearly derivable** from:
- algorithm steps,
- equations,
- procedural descriptions shown on slides.

This is especially severe for:
- *â€œwhyâ€* questions,
- algorithm explanations,
- time/space complexity discussions,
- reasoning over steps.

ğŸ‘‰ **No existing open-source framework robustly handles this for educational / lecture-style videos.**

---

## ğŸ’¡ Key Insight

> **Grounding is not binary for lecture videos.**

There are **three distinct grounding regimes**:

| Grounding Type | Example | Should Answer? |
|---------------|--------|----------------|
| **Explicit grounding** | â€œWhat is written on the slide?â€ | âœ… |
| **Derivable-from-steps grounding** | â€œWhy initialize keys to âˆ in Primâ€™s algorithm?â€ | âœ… |
| **Theoretical / external grounding** | â€œWhy does Prim always produce an MST?â€ | âŒ |

Most systems collapse everything into **SUPPORTED vs UNSUPPORTED**, which is incorrect for algorithmic lectures.

---

## ğŸ§  What This Repository Implements

This repository introduces a **Grounding-Aware Self-Refinement Framework** for lecture videos.

---

## ğŸ§© Core Components

### 1ï¸âƒ£ OCR-First Grounding

- OCR is treated as **primary evidence**, not a helper signal.
- The model is explicitly instructed to use:
  - OCR text
  - clearly visible visual evidence
- External knowledge is disallowed unless it is **logically derivable from shown steps**.

This prevents hallucinations while still allowing explanation-based answers.

---

### 2ï¸âƒ£ Hybrid OCR Retrieval (Query-Aware)

- OCR is run on uniformly sampled frames.
- A **hybrid search** (semantic similarity + keyword overlap) selects only OCR segments relevant to the question.
- This removes noise from:
  - instructor bios,
  - course outlines,
  - unrelated slides,
  - decorative content.

---

### 3ï¸âƒ£ Grounding-Aware Self-Refinement

Inspired by SELF-REFINE, but **adapted for multimodal grounding**.

Each iteration consists of:
1. **Answer generation**
2. **Grounding feedback classification**
3. **Answer refinement**

Instead of binary feedback, answers are classified into:

- `SUPPORTED`
- `DERIVABLE_FROM_STEPS`
- `PARTIALLY_SUPPORTED`
- `UNSUPPORTED`

This enables:
- correct algorithm explanations,
- grounded reasoning over steps,
- correction of partial hallucinations,
- abstention only when truly necessary.

---

### 4ï¸âƒ£ Robustness to Black-Screen & Failure Cases

- On videos with no usable OCR or visual evidence, the system **correctly abstains**.
- This prevents the common failure of confident but ungrounded answers.

---

## ğŸ§  Architecture Overview

Video
â”œâ”€ Uniform frame sampling (OCR-oriented)
â”‚
â”œâ”€ OCR extraction
â”‚
â”œâ”€ Hybrid OCR retrieval (query-aware)
â”‚
â”œâ”€ Grounded Answer Generation (Qwen2.5-VL)
â”‚
â”œâ”€ Grounding Feedback
â”‚ â”œâ”€ SUPPORTED
â”‚ â”œâ”€ DERIVABLE_FROM_STEPS
â”‚ â”œâ”€ PARTIALLY_SUPPORTED
â”‚ â””â”€ UNSUPPORTED
â”‚
â””â”€ Iterative Self-Refinement
â†“
Final Grounded Answer



---

## ğŸ”¬ Why This Is Novel

To the best of our knowledge:

- âŒ No prior open-source framework explicitly targets **lecture-style videos** with:
  - slides + blackboard + face cam
- âŒ No system properly handles **algorithmic â€œwhyâ€ questions** without hallucinating or over-abstaining
- âŒ SELF-REFINE has not been adapted for **OCR-grounded multimodal reasoning**

This work demonstrates that:

> **Strict grounding without step-derivable reasoning breaks educational QA.**

---

## ğŸ“ Repository Structure

.
â”œâ”€â”€ self_refine_framework_Qwen2_5.py # Main self-refinement pipeline
â”œâ”€â”€ hybrid_search.py # Query-aware OCR retrieval
â”œâ”€â”€ nanonetOCR/ # OCR module
â”œâ”€â”€ samples/ # Example lecture videos
â”œâ”€â”€ README.md


---

## ğŸš€ How to Run

```bash
python self_refine_framework_Qwen2_5.py


Requirements

GPU compatible with Qwen2.5-VL

Python â‰¥ 3.9

Dependencies:

transformers

decord

opencv

torch

NanoNet OCR

ğŸ§ª Example Use Cases

Algorithm explanation lectures (Primâ€™s, BFS, DFS, Sorting)

Blackboard-heavy teaching videos

MOOCs (NPTEL, Coursera, edX)

Long educational videos with sparse visual change


ğŸ”® Future Work

This repository focuses on grounded reasoning.
Planned extensions include:

ğŸ”Š Automatic Speech Recognition (ASR) integration

ğŸ¯ Query-aware frame sampling (beyond uniform OCR sampling)

ğŸ“Š Evaluation on educational video QA benchmarks

ğŸ§  Temporal reasoning across slide transitions

âš¡ Performance optimization for long videos

ğŸ“Œ Takeaway

Lecture videos are not â€œjust another video domainâ€.
They require procedural grounding, OCR-aware reasoning, and careful self-refinement.

This repository is a step toward trustworthy Video Question Answering for education.